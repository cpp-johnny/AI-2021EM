{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZdH1O21CN0t",
        "outputId": "19b8dace-bb19-4305-b76c-b0a81487d0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Collecting yahoo_fin\n",
            "  Downloading yahoo_fin-0.8.9.1-py3-none-any.whl (10 kB)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting pyquery\n",
            "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: bs4 in /usr/local/lib/python3.8/dist-packages (from requests-html->yahoo_fin) (0.0.1)\n",
            "Collecting pyppeteer>=0.0.14\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 726 kB/s \n",
            "\u001b[?25hCollecting w3lib\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "Collecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.64.1)\n",
            "Collecting websockets<11.0,>=10.0\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.4.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 27.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from bs4->requests-html->yahoo_fin) (4.6.3)\n",
            "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.8/dist-packages (from fake-useragent->requests-html->yahoo_fin) (5.10.0)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.8/dist-packages (from pyquery->requests-html->yahoo_fin) (4.9.1)\n",
            "Building wheels for collected packages: sklearn, parse, sgmllib3k\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=f6ccce248632f4b7174dde424818d9c78f7cb8adf42c66e7e0c2c749fa7e8825\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=da3c35090895ee1c743de6f07c6026cda4752ff54381598f1138cf4fa530ffcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/33/1f/68392720485b3ecf125a69e700baaab7624616deedea2fa6e2\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=7f84bea7758b4749ae41fbd6468ed461b5e3a3be8eb70ce632185a259527b2c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
            "Successfully built sklearn parse sgmllib3k\n",
            "Installing collected packages: urllib3, websockets, pyee, cssselect, w3lib, sgmllib3k, pyquery, pyppeteer, parse, fake-useragent, requests-html, feedparser, yahoo-fin, sklearn\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed cssselect-1.2.0 fake-useragent-1.1.1 feedparser-6.0.10 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-1.4.3 requests-html-0.10.0 sgmllib3k-1.0.0 sklearn-0.0.post1 urllib3-1.25.11 w3lib-2.1.1 websockets-10.4 yahoo-fin-0.8.9.1\n",
            "Epoch 1/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 0.0025 - mean_absolute_error: 0.0304\n",
            "Epoch 1: val_loss improved from inf to 0.00036, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 59s 680ms/step - loss: 0.0025 - mean_absolute_error: 0.0304 - val_loss: 3.6357e-04 - val_mean_absolute_error: 0.0131\n",
            "Epoch 2/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 7.7265e-04 - mean_absolute_error: 0.0186\n",
            "Epoch 2: val_loss did not improve from 0.00036\n",
            "80/80 [==============================] - 53s 658ms/step - loss: 7.7265e-04 - mean_absolute_error: 0.0186 - val_loss: 4.2321e-04 - val_mean_absolute_error: 0.0146\n",
            "Epoch 3/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 7.3095e-04 - mean_absolute_error: 0.0179\n",
            "Epoch 3: val_loss improved from 0.00036 to 0.00031, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 52s 646ms/step - loss: 7.3095e-04 - mean_absolute_error: 0.0179 - val_loss: 3.0951e-04 - val_mean_absolute_error: 0.0123\n",
            "Epoch 4/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 8.2124e-04 - mean_absolute_error: 0.0192\n",
            "Epoch 4: val_loss did not improve from 0.00031\n",
            "80/80 [==============================] - 53s 659ms/step - loss: 8.2124e-04 - mean_absolute_error: 0.0192 - val_loss: 0.0011 - val_mean_absolute_error: 0.0237\n",
            "Epoch 5/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.9404e-04 - mean_absolute_error: 0.0178\n",
            "Epoch 5: val_loss did not improve from 0.00031\n",
            "80/80 [==============================] - 52s 650ms/step - loss: 6.9404e-04 - mean_absolute_error: 0.0178 - val_loss: 4.0937e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 6/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 7.2439e-04 - mean_absolute_error: 0.0178\n",
            "Epoch 6: val_loss did not improve from 0.00031\n",
            "80/80 [==============================] - 52s 647ms/step - loss: 7.2439e-04 - mean_absolute_error: 0.0178 - val_loss: 3.3472e-04 - val_mean_absolute_error: 0.0113\n",
            "Epoch 7/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 8.0175e-04 - mean_absolute_error: 0.0196\n",
            "Epoch 7: val_loss improved from 0.00031 to 0.00029, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 53s 664ms/step - loss: 8.0175e-04 - mean_absolute_error: 0.0196 - val_loss: 2.8859e-04 - val_mean_absolute_error: 0.0114\n",
            "Epoch 8/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 8.0335e-04 - mean_absolute_error: 0.0195\n",
            "Epoch 8: val_loss did not improve from 0.00029\n",
            "80/80 [==============================] - 52s 652ms/step - loss: 8.0335e-04 - mean_absolute_error: 0.0195 - val_loss: 3.0655e-04 - val_mean_absolute_error: 0.0109\n",
            "Epoch 9/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 7.0452e-04 - mean_absolute_error: 0.0179\n",
            "Epoch 9: val_loss improved from 0.00029 to 0.00027, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 51s 644ms/step - loss: 7.0452e-04 - mean_absolute_error: 0.0179 - val_loss: 2.7304e-04 - val_mean_absolute_error: 0.0118\n",
            "Epoch 10/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.8657e-04 - mean_absolute_error: 0.0164\n",
            "Epoch 10: val_loss did not improve from 0.00027\n",
            "80/80 [==============================] - 54s 677ms/step - loss: 5.8657e-04 - mean_absolute_error: 0.0164 - val_loss: 3.0087e-04 - val_mean_absolute_error: 0.0109\n",
            "Epoch 11/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.9388e-04 - mean_absolute_error: 0.0163\n",
            "Epoch 11: val_loss did not improve from 0.00027\n",
            "80/80 [==============================] - 54s 664ms/step - loss: 5.9388e-04 - mean_absolute_error: 0.0163 - val_loss: 4.3023e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 12/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.2680e-04 - mean_absolute_error: 0.0173\n",
            "Epoch 12: val_loss improved from 0.00027 to 0.00027, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 53s 660ms/step - loss: 6.2680e-04 - mean_absolute_error: 0.0173 - val_loss: 2.7227e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 13/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.1206e-04 - mean_absolute_error: 0.0169\n",
            "Epoch 13: val_loss improved from 0.00027 to 0.00027, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 53s 659ms/step - loss: 6.1206e-04 - mean_absolute_error: 0.0169 - val_loss: 2.7072e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 14/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.7355e-04 - mean_absolute_error: 0.0164\n",
            "Epoch 14: val_loss did not improve from 0.00027\n",
            "80/80 [==============================] - 52s 656ms/step - loss: 5.7355e-04 - mean_absolute_error: 0.0164 - val_loss: 4.3339e-04 - val_mean_absolute_error: 0.0143\n",
            "Epoch 15/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.6144e-04 - mean_absolute_error: 0.0174\n",
            "Epoch 15: val_loss improved from 0.00027 to 0.00026, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 52s 653ms/step - loss: 6.6144e-04 - mean_absolute_error: 0.0174 - val_loss: 2.6399e-04 - val_mean_absolute_error: 0.0100\n",
            "Epoch 16/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.1926e-04 - mean_absolute_error: 0.0168\n",
            "Epoch 16: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 53s 660ms/step - loss: 6.1926e-04 - mean_absolute_error: 0.0168 - val_loss: 2.7873e-04 - val_mean_absolute_error: 0.0106\n",
            "Epoch 17/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.5987e-04 - mean_absolute_error: 0.0164\n",
            "Epoch 17: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 51s 641ms/step - loss: 5.5987e-04 - mean_absolute_error: 0.0164 - val_loss: 3.6307e-04 - val_mean_absolute_error: 0.0123\n",
            "Epoch 18/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.5432e-04 - mean_absolute_error: 0.0176\n",
            "Epoch 18: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 53s 658ms/step - loss: 6.5432e-04 - mean_absolute_error: 0.0176 - val_loss: 2.8093e-04 - val_mean_absolute_error: 0.0132\n",
            "Epoch 19/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.1274e-04 - mean_absolute_error: 0.0177\n",
            "Epoch 19: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 53s 659ms/step - loss: 6.1274e-04 - mean_absolute_error: 0.0177 - val_loss: 3.5031e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 20/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.8995e-04 - mean_absolute_error: 0.0169\n",
            "Epoch 20: val_loss improved from 0.00026 to 0.00026, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 53s 661ms/step - loss: 5.8995e-04 - mean_absolute_error: 0.0169 - val_loss: 2.5971e-04 - val_mean_absolute_error: 0.0100\n",
            "Epoch 21/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.7301e-04 - mean_absolute_error: 0.0169\n",
            "Epoch 21: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 51s 634ms/step - loss: 5.7301e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7365e-04 - val_mean_absolute_error: 0.0122\n",
            "Epoch 22/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.6022e-04 - mean_absolute_error: 0.0165\n",
            "Epoch 22: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 648ms/step - loss: 5.6022e-04 - mean_absolute_error: 0.0165 - val_loss: 2.6940e-04 - val_mean_absolute_error: 0.0101\n",
            "Epoch 23/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.9843e-04 - mean_absolute_error: 0.0169\n",
            "Epoch 23: val_loss improved from 0.00026 to 0.00026, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 55s 691ms/step - loss: 5.9843e-04 - mean_absolute_error: 0.0169 - val_loss: 2.5852e-04 - val_mean_absolute_error: 0.0104\n",
            "Epoch 24/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.6713e-04 - mean_absolute_error: 0.0168\n",
            "Epoch 24: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 51s 641ms/step - loss: 5.6713e-04 - mean_absolute_error: 0.0168 - val_loss: 3.8299e-04 - val_mean_absolute_error: 0.0129\n",
            "Epoch 25/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.8741e-04 - mean_absolute_error: 0.0173\n",
            "Epoch 25: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 53s 659ms/step - loss: 5.8741e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0698e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 26/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.9598e-04 - mean_absolute_error: 0.0173\n",
            "Epoch 26: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 645ms/step - loss: 5.9598e-04 - mean_absolute_error: 0.0173 - val_loss: 4.3291e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 27/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.5294e-04 - mean_absolute_error: 0.0168\n",
            "Epoch 27: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 651ms/step - loss: 5.5294e-04 - mean_absolute_error: 0.0168 - val_loss: 3.9743e-04 - val_mean_absolute_error: 0.0138\n",
            "Epoch 28/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.6572e-04 - mean_absolute_error: 0.0185\n",
            "Epoch 28: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 651ms/step - loss: 6.6572e-04 - mean_absolute_error: 0.0185 - val_loss: 2.6641e-04 - val_mean_absolute_error: 0.0100\n",
            "Epoch 29/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.6408e-04 - mean_absolute_error: 0.0169\n",
            "Epoch 29: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 53s 669ms/step - loss: 5.6408e-04 - mean_absolute_error: 0.0169 - val_loss: 2.7779e-04 - val_mean_absolute_error: 0.0107\n",
            "Epoch 30/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.2022e-04 - mean_absolute_error: 0.0164\n",
            "Epoch 30: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 53s 661ms/step - loss: 5.2022e-04 - mean_absolute_error: 0.0164 - val_loss: 2.9327e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 31/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.3886e-04 - mean_absolute_error: 0.0167\n",
            "Epoch 31: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 648ms/step - loss: 5.3886e-04 - mean_absolute_error: 0.0167 - val_loss: 2.5868e-04 - val_mean_absolute_error: 0.0105\n",
            "Epoch 32/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.5342e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 32: val_loss improved from 0.00026 to 0.00026, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 54s 677ms/step - loss: 5.5342e-04 - mean_absolute_error: 0.0171 - val_loss: 2.5734e-04 - val_mean_absolute_error: 0.0103\n",
            "Epoch 33/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.0420e-04 - mean_absolute_error: 0.0176\n",
            "Epoch 33: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 54s 676ms/step - loss: 6.0420e-04 - mean_absolute_error: 0.0176 - val_loss: 3.4637e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 34/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.4677e-04 - mean_absolute_error: 0.0173\n",
            "Epoch 34: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 54s 680ms/step - loss: 5.4677e-04 - mean_absolute_error: 0.0173 - val_loss: 2.8999e-04 - val_mean_absolute_error: 0.0122\n",
            "Epoch 35/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.1948e-04 - mean_absolute_error: 0.0180\n",
            "Epoch 35: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 51s 635ms/step - loss: 6.1948e-04 - mean_absolute_error: 0.0180 - val_loss: 2.6474e-04 - val_mean_absolute_error: 0.0114\n",
            "Epoch 36/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.3963e-04 - mean_absolute_error: 0.0168\n",
            "Epoch 36: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 53s 660ms/step - loss: 5.3963e-04 - mean_absolute_error: 0.0168 - val_loss: 2.8545e-04 - val_mean_absolute_error: 0.0111\n",
            "Epoch 37/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.4295e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 37: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 652ms/step - loss: 5.4295e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1278e-04 - val_mean_absolute_error: 0.0118\n",
            "Epoch 38/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.4617e-04 - mean_absolute_error: 0.0175\n",
            "Epoch 38: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 54s 674ms/step - loss: 5.4617e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2574e-04 - val_mean_absolute_error: 0.0109\n",
            "Epoch 39/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.8961e-04 - mean_absolute_error: 0.0178\n",
            "Epoch 39: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 657ms/step - loss: 5.8961e-04 - mean_absolute_error: 0.0178 - val_loss: 2.7029e-04 - val_mean_absolute_error: 0.0103\n",
            "Epoch 40/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.6083e-04 - mean_absolute_error: 0.0174\n",
            "Epoch 40: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 53s 659ms/step - loss: 5.6083e-04 - mean_absolute_error: 0.0174 - val_loss: 2.7753e-04 - val_mean_absolute_error: 0.0108\n",
            "Epoch 41/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.7130e-04 - mean_absolute_error: 0.0178\n",
            "Epoch 41: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 645ms/step - loss: 5.7130e-04 - mean_absolute_error: 0.0178 - val_loss: 2.9991e-04 - val_mean_absolute_error: 0.0108\n",
            "Epoch 42/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.0409e-04 - mean_absolute_error: 0.0170\n",
            "Epoch 42: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 54s 681ms/step - loss: 5.0409e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8042e-04 - val_mean_absolute_error: 0.0125\n",
            "Epoch 43/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.3522e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 43: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 653ms/step - loss: 5.3522e-04 - mean_absolute_error: 0.0171 - val_loss: 4.7417e-04 - val_mean_absolute_error: 0.0160\n",
            "Epoch 44/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.6371e-04 - mean_absolute_error: 0.0176\n",
            "Epoch 44: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 52s 651ms/step - loss: 5.6371e-04 - mean_absolute_error: 0.0176 - val_loss: 2.6941e-04 - val_mean_absolute_error: 0.0112\n",
            "Epoch 45/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.9566e-04 - mean_absolute_error: 0.0182\n",
            "Epoch 45: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 51s 635ms/step - loss: 5.9566e-04 - mean_absolute_error: 0.0182 - val_loss: 2.9112e-04 - val_mean_absolute_error: 0.0131\n",
            "Epoch 46/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 6.2492e-04 - mean_absolute_error: 0.0190\n",
            "Epoch 46: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 54s 674ms/step - loss: 6.2492e-04 - mean_absolute_error: 0.0190 - val_loss: 2.5999e-04 - val_mean_absolute_error: 0.0104\n",
            "Epoch 47/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.4046e-04 - mean_absolute_error: 0.0173\n",
            "Epoch 47: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 54s 677ms/step - loss: 5.4046e-04 - mean_absolute_error: 0.0173 - val_loss: 2.5794e-04 - val_mean_absolute_error: 0.0106\n",
            "Epoch 48/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.2704e-04 - mean_absolute_error: 0.0174\n",
            "Epoch 48: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 55s 693ms/step - loss: 5.2704e-04 - mean_absolute_error: 0.0174 - val_loss: 2.7131e-04 - val_mean_absolute_error: 0.0099\n",
            "Epoch 49/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.4975e-04 - mean_absolute_error: 0.0178\n",
            "Epoch 49: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 54s 673ms/step - loss: 5.4975e-04 - mean_absolute_error: 0.0178 - val_loss: 2.6230e-04 - val_mean_absolute_error: 0.0100\n",
            "Epoch 50/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.0625e-04 - mean_absolute_error: 0.0170\n",
            "Epoch 50: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 56s 703ms/step - loss: 5.0625e-04 - mean_absolute_error: 0.0170 - val_loss: 2.6237e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 51/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.7936e-04 - mean_absolute_error: 0.0180\n",
            "Epoch 51: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 55s 688ms/step - loss: 5.7936e-04 - mean_absolute_error: 0.0180 - val_loss: 2.6272e-04 - val_mean_absolute_error: 0.0113\n",
            "Epoch 52/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.5095e-04 - mean_absolute_error: 0.0177\n",
            "Epoch 52: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 55s 684ms/step - loss: 5.5095e-04 - mean_absolute_error: 0.0177 - val_loss: 3.0999e-04 - val_mean_absolute_error: 0.0117\n",
            "Epoch 53/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.1217e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 53: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 55s 686ms/step - loss: 5.1217e-04 - mean_absolute_error: 0.0171 - val_loss: 3.0199e-04 - val_mean_absolute_error: 0.0106\n",
            "Epoch 54/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.1668e-04 - mean_absolute_error: 0.0176\n",
            "Epoch 54: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 57s 708ms/step - loss: 5.1668e-04 - mean_absolute_error: 0.0176 - val_loss: 2.6241e-04 - val_mean_absolute_error: 0.0105\n",
            "Epoch 55/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.4801e-04 - mean_absolute_error: 0.0182\n",
            "Epoch 55: val_loss did not improve from 0.00026\n",
            "80/80 [==============================] - 55s 690ms/step - loss: 5.4801e-04 - mean_absolute_error: 0.0182 - val_loss: 3.0328e-04 - val_mean_absolute_error: 0.0112\n",
            "Epoch 56/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.2210e-04 - mean_absolute_error: 0.0174\n",
            "Epoch 56: val_loss improved from 0.00026 to 0.00025, saving model to results/2022-12-14_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "80/80 [==============================] - 55s 689ms/step - loss: 5.2210e-04 - mean_absolute_error: 0.0174 - val_loss: 2.5071e-04 - val_mean_absolute_error: 0.0101\n",
            "Epoch 57/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.1667e-04 - mean_absolute_error: 0.0173\n",
            "Epoch 57: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 55s 694ms/step - loss: 5.1667e-04 - mean_absolute_error: 0.0173 - val_loss: 2.5249e-04 - val_mean_absolute_error: 0.0109\n",
            "Epoch 58/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.5832e-04 - mean_absolute_error: 0.0178\n",
            "Epoch 58: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 54s 677ms/step - loss: 5.5832e-04 - mean_absolute_error: 0.0178 - val_loss: 3.5005e-04 - val_mean_absolute_error: 0.0121\n",
            "Epoch 59/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.8334e-04 - mean_absolute_error: 0.0187\n",
            "Epoch 59: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 53s 667ms/step - loss: 5.8334e-04 - mean_absolute_error: 0.0187 - val_loss: 2.7867e-04 - val_mean_absolute_error: 0.0111\n",
            "Epoch 60/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.0075e-04 - mean_absolute_error: 0.0173\n",
            "Epoch 60: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 52s 650ms/step - loss: 5.0075e-04 - mean_absolute_error: 0.0173 - val_loss: 2.7007e-04 - val_mean_absolute_error: 0.0108\n",
            "Epoch 61/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.1218e-04 - mean_absolute_error: 0.0178\n",
            "Epoch 61: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 55s 683ms/step - loss: 5.1218e-04 - mean_absolute_error: 0.0178 - val_loss: 2.7600e-04 - val_mean_absolute_error: 0.0113\n",
            "Epoch 62/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.1064e-04 - mean_absolute_error: 0.0174\n",
            "Epoch 62: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 54s 672ms/step - loss: 5.1064e-04 - mean_absolute_error: 0.0174 - val_loss: 2.5071e-04 - val_mean_absolute_error: 0.0111\n",
            "Epoch 63/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.2856e-04 - mean_absolute_error: 0.0176\n",
            "Epoch 63: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 54s 673ms/step - loss: 5.2856e-04 - mean_absolute_error: 0.0176 - val_loss: 2.8127e-04 - val_mean_absolute_error: 0.0104\n",
            "Epoch 64/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 5.4961e-04 - mean_absolute_error: 0.0177\n",
            "Epoch 64: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 54s 677ms/step - loss: 5.4961e-04 - mean_absolute_error: 0.0177 - val_loss: 2.6484e-04 - val_mean_absolute_error: 0.0112\n",
            "Epoch 65/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 4.8911e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 65: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 55s 684ms/step - loss: 4.8911e-04 - mean_absolute_error: 0.0171 - val_loss: 3.4450e-04 - val_mean_absolute_error: 0.0131\n",
            "Epoch 66/500\n",
            "80/80 [==============================] - ETA: 0s - loss: 4.9940e-04 - mean_absolute_error: 0.0171\n",
            "Epoch 66: val_loss did not improve from 0.00025\n",
            "80/80 [==============================] - 55s 687ms/step - loss: 4.9940e-04 - mean_absolute_error: 0.0171 - val_loss: 2.5461e-04 - val_mean_absolute_error: 0.0101\n",
            "Epoch 67/500\n",
            "76/80 [===========================>..] - ETA: 2s - loss: 5.1700e-04 - mean_absolute_error: 0.0174"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorflow pandas numpy matplotlib yahoo_fin sklearn\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from yahoo_fin import stock_info as si\n",
        "from collections import deque\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# set seed, so we can get the same results after rerunning several times\n",
        "np.random.seed(314)\n",
        "tf.random.set_seed(314)\n",
        "random.seed(314)\n",
        "def shuffle_in_unison(a, b):\n",
        "    # shuffle two arrays in the same way\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(b)\n",
        "\n",
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
        "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "\n",
        "    \"\"\"\n",
        "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
        "    Params:\n",
        "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
        "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
        "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
        "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
        "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
        "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it\n",
        "            to False will split datasets in a random way\n",
        "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
        "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
        "    \"\"\"\n",
        "\n",
        "    # see if ticker is already a loaded stock from yahoo finance\n",
        "    if isinstance(ticker, str):\n",
        "        # load it from yahoo_fin library\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        # already loaded, use it directly\n",
        "        df = ticker\n",
        "    else:\n",
        "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
        "    # add date as a column\n",
        "    if \"date\" not in df.columns:\n",
        "        df[\"date\"] = df.index\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
        "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
        "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    if split_by_date:\n",
        "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
        "        train_samples = int((1 - test_size) * len(X))\n",
        "        result[\"X_train\"] = X[:train_samples]\n",
        "        result[\"y_train\"] = y[:train_samples]\n",
        "        result[\"X_test\"]  = X[train_samples:]\n",
        "        result[\"y_test\"]  = y[train_samples:]\n",
        "        if shuffle:\n",
        "            # shuffle the datasets for training (if shuffle parameter is set)\n",
        "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
        "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
        "    else:\n",
        "        # split the dataset randomly\n",
        "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y,\n",
        "                                                                                test_size=test_size, shuffle=shuffle)\n",
        "    # get the list of test set dates\n",
        "    dates = result[\"X_test\"][:, -1, -1]\n",
        "    # retrieve test features from the original dataframe\n",
        "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
        "    # remove duplicated dates in the testing dataframe\n",
        "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
        "    # remove dates from the training/testing sets & convert to float32\n",
        "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    return result\n",
        "\n",
        "# create model\n",
        "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# training the model\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Window size or the sequence length\n",
        "N_STEPS = 50\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 15\n",
        "# whether to scale feature columns & output price as well\n",
        "SCALE = True\n",
        "scale_str = f\"sc-{int(SCALE)}\"\n",
        "# whether to shuffle the dataset\n",
        "SHUFFLE = True\n",
        "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
        "# whether to split the training/testing set by date\n",
        "SPLIT_BY_DATE = False\n",
        "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 2\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "# whether to use bidirectional RNNs\n",
        "BIDIRECTIONAL = False\n",
        "### training parameters\n",
        "# mean absolute error loss\n",
        "# LOSS = \"mae\"\n",
        "# huber loss\n",
        "LOSS = \"huber_loss\"\n",
        "OPTIMIZER = \"adam\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 500\n",
        "# Amazon stock market\n",
        "ticker = \"AMZN\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save, making it as unique as possible based on parameters\n",
        "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
        "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "if BIDIRECTIONAL:\n",
        "    model_name += \"-b\"\n",
        "\n",
        "    # create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")\n",
        "\n",
        "    # load the data\n",
        "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE,\n",
        "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
        "                feature_columns=FEATURE_COLUMNS)\n",
        "# save the dataframe\n",
        "data[\"df\"].to_csv(ticker_data_filename)\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "# train the model and save the weights whenever we see\n",
        "# a new optimal model using ModelCheckpoint\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "# testing the model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graph(test_df):\n",
        "    \"\"\"\n",
        "    This function plots true close price along with predicted close price\n",
        "    with blue and red colors respectively\n",
        "    \"\"\"\n",
        "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
        "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
        "    plt.show()\n",
        "\n",
        "def get_final_df(model, data):\n",
        "    \"\"\"\n",
        "    This function takes the `model` and `data` dict to\n",
        "    construct a final dataframe that includes the features along\n",
        "    with true and predicted prices of the testing dataset\n",
        "    \"\"\"\n",
        "    # if predicted future price is higher than the current,\n",
        "    # then calculate the true future price minus the current price, to get the buy profit\n",
        "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
        "    # if the predicted future price is lower than the current price,\n",
        "    # then subtract the true future price from the current price\n",
        "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_test = data[\"y_test\"]\n",
        "    # perform prediction and get prices\n",
        "    y_pred = model.predict(X_test)\n",
        "    if SCALE:\n",
        "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    test_df = data[\"test_df\"]\n",
        "    # add predicted future prices to the dataframe\n",
        "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
        "    # add true future prices to the dataframe\n",
        "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
        "    # sort the dataframe by date\n",
        "    test_df.sort_index(inplace=True)\n",
        "    final_df = test_df\n",
        "    # add the buy profit column\n",
        "    final_df[\"buy_profit\"] = list(map(buy_profit,\n",
        "                                    final_df[\"adjclose\"],\n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"],\n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    # add the sell profit column\n",
        "    final_df[\"sell_profit\"] = list(map(sell_profit,\n",
        "                                    final_df[\"adjclose\"],\n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"],\n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    return final_df\n",
        "\n",
        "\n",
        "def predict(model, data):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    if SCALE:\n",
        "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "    else:\n",
        "        predicted_price = prediction[0][0]\n",
        "    return predicted_price\n",
        "\n",
        "# load optimal model weights from results folder\n",
        "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
        "model.load_weights(model_path)\n",
        "\n",
        "# evaluate the model\n",
        "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "if SCALE:\n",
        "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
        "else:\n",
        "    mean_absolute_error = mae\n",
        "\n",
        "# get the final dataframe for the testing set\n",
        "final_df = get_final_df(model, data)\n",
        "\n",
        "# predict the future price\n",
        "future_price = predict(model, data)\n",
        "\n",
        "# we calculate the accuracy by counting the number of positive profits\n",
        "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
        "# calculating total buy & sell profit\n",
        "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
        "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
        "# total profit by adding sell & buy together\n",
        "total_profit = total_buy_profit + total_sell_profit\n",
        "# dividing total profit by number of testing samples (number of trades)\n",
        "profit_per_trade = total_profit / len(final_df)\n",
        "\n",
        "# printing metrics\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
        "print(f\"{LOSS} loss:\", loss)\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
        "print(\"Accuracy score:\", accuracy_score)\n",
        "print(\"Total buy profit:\", total_buy_profit)\n",
        "print(\"Total sell profit:\", total_sell_profit)\n",
        "print(\"Total profit:\", total_profit)\n",
        "print(\"Profit per trade:\", profit_per_trade)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}